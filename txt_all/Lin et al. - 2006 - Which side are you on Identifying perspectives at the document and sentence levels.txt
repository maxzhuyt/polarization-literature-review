Automatic classification of citation function

             Simone Teufel     Advaith Siddharthan       Dan Tidhar
                Natural Language and Information Processing Group
                              Computer Laboratory
                       Cambridge University, CB3 0FD, UK
    {Simone.Teufel,Advaith.Siddharthan,Dan.Tidhar}@cl.cam.ac.uk


                      Abstract                                ing should not “count” as much as central citations in a paper, or as those citations where a reCitation function is defined as the author’s              searcher’s work is used as the starting point of
    reason for citing a given paper (e.g. ac-                 somebody else’s work (Bonzi, 1982). A plethora
    knowledgement of the use of the cited                     of manual annotation schemes for citation motivamethod). The automatic recognition of the                 tion have been invented over the years (Garfield,
    rhetorical function of citations in scientific            1979; Hodges, 1972; Chubin and Moitra, 1975).
    text has many applications, from improve-                 Other schemes concentrate on citation function
    ment of impact factor calculations to text                (Spiegel-Rüsing, 1977; O’Connor, 1982; Weinsummarisation and more informative ci-                    stock, 1971; Swales, 1990; Small, 1982)). One
    tation indexers. We show that our anno-                   of the best-known of these studies (Moravcsik
    tation scheme for citation function is re-                and Murugesan, 1975) divides citations in running
    liable, and present a supervised machine                  text into four dimensions: conceptual or operalearning framework to automatically clas-                 tional use (i.e., use of theory vs. use of technical
    sify citation function, using both shallow                method); evolutionary or juxtapositional (i.e., own
    and linguistically-inspired features. We                  work is based on the cited work vs. own work is an
    find, amongst other things, a strong re-                  alternative to it); organic or perfunctory (i.e., work
    lationship between citation function and                  is crucially needed for understanding of citing arsentiment classification.                                 ticle or just a general acknowledgement); and finally confirmative vs. negational (i.e., is the cor1 Introduction                                                rectness of the findings disputed?). They found,
Why do researchers cite a particular paper? This              for example, that 40% of the citations were peris a question that has interested researchers in              functory, which casts further doubt on the citationdiscourse analysis, sociology of science, and in-             counting approach.
formation sciences (library sciences) for decades                Based on such annotation schemes and hand-
(Garfield, 1979; Small, 1982; White, 2004). Many              analyzed data, different influences on citation beannotation schemes for citation motivation have               haviour can be determined. Nevertheless, rebeen created over the years, and the question has             searchers in the field of citation content analysis
been studied in detail, even to the level of in-depth         do not normally cross-validate their schemes with
interviews with writers about each individual cita-           independent annotation studies with other human
tion (Hodges, 1972).                                          annotators, and usually only annotate a small numPart of this sustained interest in citations can           ber of citations (in the range of hundreds or thoube explained by the fact that bibliometric met-               sands). Also, automated application of the annotarics are commonly used to measure the impact of               tion is not something that is generally considered
a researcher’s work by how often they are cited               in the field, though White (2004) sees the future of
(Borgman, 1990; Luukkonen, 1992). However, re-                discourse-analytic citation analysis in automation.
searchers from the field of discourse studies have               Apart from raw material for bibliometric studlong criticised purely quantitative citation analy-           ies, citations can also be used for search purposes
sis, pointing out that many citations are done out            in document retrieval applications. In the library
of “politeness, policy or piety” (Ziman, 1968),               world, printed or electronic citation indexes such
and that criticising citations or citations in pass-          as ISI (Garfield, 1979) serve as an orthogonal
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 103–110,
                          Sydney, July 2006. c 2006 Association for Computational Linguistics


 Li and Abe 96                   Brown et al. 90a       Church and Gale 91              can fulfil the relation search need: it is always
            Resnik 95                          Rose et al. 90                           centered around the physical location of the citations, but the context is often not informative
                                                                Dagan et al. 94
                                             Dagan et al 93
                                                                                        enough for the searcher to infer the relation. In
Hindle 93            Nitta and Niwa 94
                                                                                        fact, studies from our annotated corpus (Teufel,
                                                                                        1999) show that 69% of the 600 sentences stating contrast with other work and 21% of the
Hindle 90
                                                                                        246 sentences stating research continuation with
                             Pereira et al. 93                                          other work do not contain the corresponding citaHis notion of similarity
    seems to agree with our              Following Pereira et al, we measure            tion; the citation is found in preceding sentences
    intuitions in many cases,            word similarity by the relative entropy
    but it is not clear how it           or Kulbach−Leibler (KL) distance, bet−         (which means that the sentence expressing the
    can be used directly to
    construct word classes
                                         ween the corresponding conditional
                                         distributions.
                                                                                        contrast or continuation is outside the CiteSeer
    and corresponding
    models of association.                                                              snippet). A more sophisticated, discourse-aware
                                                                                        citation indexer which finds these sentences and
            Figure 1: A rhetorical citation map                                         associates them with the citation would add considerable value to the researcher’s bibliographic
                                                                                        search (Ritchie et al., 2006b).
search tool to find relevant papers, starting from a                                       Our annotation scheme for citations is based
source paper of interest. With the increased avail-                                     on empirical work in content citation analysis. It
ability of documents in electronic form in recent                                       is designed for information retrieval applications
years, citation-based search and automatic citation                                     such as improved citation indexing and better bibindexing have become highly popular, cf. the suc-                                       liometric measures (Teufel et al., 2006). Its 12 catcessful search tools Google Scholar and CiteSeer                                        egories mark relationships with other works. Each
(Giles et al., 1998).1                                                                  citation is labelled with exactly one category. The
   But not all search needs are fulfilled by current                                    following top-level four-way distinction applies:
citation indexers. Experienced researchers are often interested in relations between articles (Shum,                                       • Explicit statement of weakness
1998). They want to know if a certain article crit-                                       • Contrast or comparison with other work (4
icises another and what the criticism is, or if the                                         categories)
current work is based on that prior work. This
                                                                                          • Agreement/usage/compatibility with other
type of information is hard to come by with current
                                                                                            work (6 categories), and
search technology. Neither the author’s abstract,
nor raw citation counts help users in assessing the                                       • A neutral category.
relation between articles.
                                                                                           In this paper, we show that the scheme can be
   Fig. 1 shows a hypothetical search tool which
                                                                                        reliably annotated by independent coders. We also
displays differences and similarities between a tarreport results of a supervised machine learning exget paper (here: Pereira et al., 1993) and the paperiment which replicates the human annotation.
pers that it cites and that cite it. Contrastive links
are shown in grey – links to rival papers and pa-                                       2 An annotation scheme for citations
pers the current paper contrasts itself to. Continuative links are shown in black – links to papers that                                   Our scheme (given in Fig. 2) is adapted from that
use the methodology of the current paper. Fig. 1                                        of Spiegel-Rüsing (1977) after an analysis of a
also displays the most characteristic textual sen-                                      corpus of scientific articles in computational lintence about each citation. For instance, we can see                                     guistics. We avoid sociologically orientated diswhich aspect of Hindle (1990) our example paper                                         tinctions (“paying homage to pioneers”), as they
criticises, and in which way the example paper’s                                        can be difficult to operationalise without deep
work was used by Dagan et al. (1994).                                                   knowledge of the field and its participants (Swales,
   Note that not even the CiteSeer text snippet                                         1986). Our redefinition of the categories aims at
                                                                                        reliably annotation; at the same time, the cateThese tools automatically citation-index all scientific ar-                       gories should be informative enough for the docuticles reached by a web-crawler, making them available to
searchers via authors or keywords in the title, and displaying                          ment management application sketched in the inthe citation in context of a text snippet.                                              troduction.
 Category   Description                                          or vice versa, we assign the category PSup. We
 Weak       Weakness of cited approach
 CoCoGM     Contrast/Comparison in Goals or Meth-                also mark similarity of (an aspect of) the approach
            ods(neutral)                                         to the cited work (PSim), and motivation of apCoCo-      Author’s work is stated to be superior to            proach used or problem addressed (PMot).
            cited work
 CoCoR0     Contrast/Comparison in Results (neutral)                Our twelfth category, Neut, bundles truly neuCoCoXY     Contrast between 2 cited methods
                                                                 tral descriptions of cited work with those cases
 PBas       Author uses cited work as basis or starting
            point                                                where the textual evidence for a citation function
 PUse       Author                                 uses          was not enough to warrant annotation of that catetools/algorithms/data/definitions
 PModi      Author        adapts       or       modifies
                                                                 gory, and all other functions for which our scheme
            tools/algorithms/data                                did not provide a specific category.
 PMot       This citation is positive about approach
            used or problem addressed (used to mo-                  Citation function is hard to annotate because it
            tivate work in current paper)                        in principle requires interpretation of author intenPSim       Author’s work and cited work are similar             tions (what could the author’s intention have been
 PSup       Author’s work and cited work are compatible/provide support for each other                  in choosing a certain citation?). One of our most
 Neut       Neutral description of cited work, or not            fundamental principles is thus to only mark explicenough textual evidence for above cate-              itly signalled citation functions. Our guidelines
            gories, or unlisted citation function
                                                                 explicitly state that a general linguistic phrase such
Figure 2: Annotation scheme for citation function.               as “better” or “used by us” must be present; this
                                                                 increases the objectivity of defining citation function. Annotators must be able to point to textual
   Our categories are as follows: One category                   evidence for assigning a particular function (and
(Weak) is reserved for weakness of previous re-                  are asked to type the source of this evidence into
search, if it is addressed by the authors. The next              the annotation tool for each citation). Categories
four categories describe comparisons or contrasts                are defined in terms of certain objective types of
between own and other work. The difference be-                   statements (e.g., there are 7 cases for PMot, e.g.
tween them concerns whether the contrast is be-                  “Citation claims that or gives reasons for why
tween methods employed or goals (CoCoGM), or                     problem Y is hard”). Annotators can use general
results, and in the case of results, a difference is             text interpretation principles when assigning the
made between the cited results being worse than                  categories (such as anaphora resolution and parthe current work (CoCo-), or comparable or bet-                  allel constructions), but are not allowed to use inter results (CoCoR0). As well as considering dif-                depth knowledge of the field or of the authors.
ferences between the current work and other work,                   Guidelines (25 pages, ∼ 150 rules) describe the
we also mark citations if they are explicitly com-               categories with examples, provide a decision tree
pared and contrasted with other work (i.e. not                   and give decision aids in systematically ambiguthe work in the current paper). This is expressed                ous cases. Nevertheless, subjective judgement of
in category CoCoXY. While this is not typically                  the annotators is still necessary to assign a single
annotated in the literature, we expect a potential               tag in an unseen context, because of the many difpractical benefit of this category for our applica-              ficult cases for annotation. Some of these concern
tion, particularly in searches for differences and               the fact that authors do not always state their purrival approaches.                                                pose clearly. For instance, several earlier studies
   The next set of categories we propose concerns                found that negational citations are rare (Moravcpositive sentiment expressed towards a citation, or              sik and Murugesan, 1975; Spiegel-Rüsing, 1977);
a statement that the other work is actively used                 MacRoberts and MacRoberts (1984) argue that the
in the current work (which we consider the ulti-                 reason for this is that they are potentially politimate praise). We mark statements of use of data                  cally dangerous. In our data we found ample eviand methods of the cited work, differentiating un-               dence of the “meekness” effect. Other difficulties
changed use (PUse) from use with adaptations                     concern the distinction of the usage of a method
(PModi). Work which is stated as the explicit                    from statements of similarity between a method
starting point or intellectual ancestry is marked                and the own method (i.e., the choice between catwith our category PBas. If a claim in the liter-                 egories PSim and PUse). This happens in cases
ature is used to strengthen the authors’ argument,               where authors do not want to admit (or stress)
that they are using somebody else’s method. An-                        that they have contributed something which is betother difficult distinction concerns the judgement                     ter or at least new (Myers, 1992)), and they thus
of whether the authors continue somebody’s re-                         have a stance towards their citations. But although
search (i.e., consider their research as intellectual                  there is a sentiment aspect to the interpretation of
ancestry, i.e. PBas), or whether they simply use                       citations, this is not the whole story. Many of our
the work (PUse).                                                       “positive” categories are more concerned with difThe unit of annotation is a) the full citation (as                  ferent ways in which the cited work is useful to the
recognised by our automatic citation processor on                      current work (which aspect of it is used, e.g., just a
our corpus), and b) names of authors of cited pa-                      definition or the entire solution?), and many of the
pers anywhere in running text outside of a for-                        contrastive statements have no negative connotamal citation context (i.e., without date). These                       tion at all and simply state a (value-free) differlatter are marked up, slightly unusually in com-                       ence between approaches. However, if one looks
parison to other citation indexers, because we be-                     at the distribution of positive and negative adjeclieve they function as important referents compa-                      tives around citations, it is clear that there is a nonrable in importance to formal citations. 2 In prin-                    trivial connection between our task and sentiment
ciple, there are many other linguistic expressions                     classification.
by which the authors could refer to other people’s                        The data we use comes from our corpus of
work: pronouns, abbreviations such as “Mueller                         360 conference articles in computational linguisand Sag (1990), henceforth M & S”, and names of                        tics, drawn from the Computation and Language
approaches or theories which are associated with                       E-Print Archive (http://xxx.lanl.gov/cmp-lg). The
particular authors. The fact that in these contexts                    articles are transformed into XML format; headcitation function cannot be annotated (because it                      lines, titles, authors and reference list items are auis not technically feasible to recognise them well                     tomatically marked up. Reference lists are parsed
enough) sometimes causes problems with context                         using regular patterns, and cited authors’ names
dependencies.                                                          are identified. Our citation parser then finds citaWhile there are unambiguous example cases                           tions and author names in running text and marks
where the citation function can be decided on the                      them up. Ritchie et al. (2006a) report high acbasis of the sentence alone, this is not always the                    curacy for this task (94% of citations recognised,
case. Most approaches are not criticised in the                        provided the reference list was error-free). On avsame sentence where they are also cited: it is more                    erage, our papers contain 26.8 citation instances in
likely that there are several descriptive sentences                    running text3 . For human annotation, we use our
about a cited approach between its formal cita-                        own annotation tool based on XML/XSLT techtion and the evaluative statement, which is often at                   nology, which allows us to use a web browser to
the end of the textual segment about this citation.                    interactively assign one of the 12 tags (presented
Nevertheless, the annotator must mark the func-                        as a pull-down list) to each citation.
tion on the nearest appropriate annotation unit (ci-                      We measure inter-annotator agreement between
tation or author name). Our rules decree that con-                     three annotators (the three authors), who indepentext is in most cases constrained to the paragraph                     dently annotated 26 articles with the scheme (conboundary. In rare cases, paper-wide information                        taining a total of 120,000 running words and 548
is required (e.g., for PMot, we need to know that                      citations), using the written guidelines. The guidea praised approach is used by the authors, infor-                      lines were developed on a different set of articles
mation which may not be local in the paragraph).                       from the ones used for annotation.
Annotators are thus asked to skim-read the paper                          Inter-annotator agreement was Kappa=.72
before annotation.                                                     (n=12;N=548;k=3)4 . This is quite high, considerOne possible view on this annotation scheme                         ing the number of categories and the difficulties
could consider the first two sets of categories as                        3
                                                                            As opposed to reference list items, which are fewer.
“negative” and the third set of categories “posi-                         4
                                                                            Following Carletta (1996), we measure agreement in
tive”, in the sense of Pang et al. (2002) and Turney                   Kappa, which follows the formula K = P (A)−P       (E)
                                                                                                                    1−P (E)
                                                                                                                              where
                                                                       P(A) is observed, and P(E) expected agreement. Kappa
(2002). Authors need to make a point (namely,                          ranges between -1 and 1. K=0 means agreement is only as
                                                                       expected by chance. Generally, Kappas of 0.8 are considered
      Our citation processor can recognise these after parsing         stable, and Kappas of .69 as marginally stable, according to
the citation list.                                                     the strictest scheme applied in the field.
(e.g., non-local dependencies) of the task. The                       authors of the paper, and everybody else) are modrelative frequency of each category observed in                       elled by 185 patterns. For instance, in a paragraph
the annotation is listed in Fig. 3. As expected,                      describing related work, we expect to find referthe distribution is very skewed, with more than                       ences to other people in subject position more of60% of the citations of category Neut. 5 What                         ten than in the section detailing the authors’ own
is interesting is the relatively high frequency                       methods, whereas in the background section, we
of usage categories (PUse, PModi, PBas)                               often find general subjects such as “researchers in
with a total of 18.9%. There is a relatively low                      computational linguistics” or “in the literature”.
frequency of clearly negative citations (Weak,                        For each sentence to be classified, its grammatical
CoCo-, total of 4.1%), whereas the neutral–                           subject is determined by POS patterns and, if poscontrastive categories (CoCoR0, CoCoXY,                               sible, classified as one of these agent types. We
CoCoGM) are slightly more frequent at 7.6%.                           also use the observation that in sentences without
This is in concordance with earlier annotation                        meta-discourse, one can assume that agenthood
experiments (Moravcsik and Murugesan, 1975;                           has not changed.
Spiegel-Rüsing, 1977).                                                  20 different action types model the main verbs
                                                                      involved in meta-discourse. For instance, there is
3 Features for automatic recognition of                               a set of verbs that is often used when the overcitation function                                                   all scientific goal of a paper is defined. These
This section summarises the features we use for                       are the verbs of presentation, such as “propose,
machine learning citation function. Some of these                     present, report” and “suggest”; in the corpus we
features were previously found useful for a dif-                      found other verbs in this function, but with a lower
ferent application, namely Argumentative Zoning                       frequency, namely “describe, discuss, give, intro-
(Teufel, 1999; Teufel and Moens, 2002), some are                      duce, put forward, show, sketch, state” and “talk
specific to citation classification.                                  about”. There are also specialised verb clusters
                                                                      which co-occur with PBas sentences, e.g., the
3.1   Cue phrases                                                     cluster of continuation of ideas (eg. “adopt, agree
                                                                      with, base, be based on, be derived from, be origMyers (1992) calls meta-discourse the set of exinated in, be inspired by, borrow, build on,. . . ”).
pressions that talk about the act of presenting reOn the other hand, the semantics of verbs in Weak
search in a paper, rather than the research itself
                                                                      sentences is often concerned with failing (of other
(which is called object-level discourse). For inresearchers’ approaches), and often contain verbs
stance, Swales (1990) names phrases such as “to
                                                                      such as “abound, aggravate, arise, be cursed, be
our knowledge, no. . . ” or “As far as we aware” as
                                                                      incapable of, be forced to, be limited to, . . . ”.
meta-discourse associated with a gap in the current literature. Strings such as these have been                         We use 20 manually acquired verb clusters.
used in extractive summarisation successfully ever                    Negation is recognised, but too rare to define its
since Paice’s (1981) work.                                            own clusters: out of the 20 × 2 = 40 theoretically
   We model meta-discourse (cue phrases) and                          possible verb clusters, only 27 were observed in
treat it differently from object-level discourse.                     our development corpus. We have recently autoThere are two different mechanisms: A finite                          mated the process of verb–object pair acquisition
grammar over strings with a placeholder mecha-                        from corpora for two types of cue phrases (Abdalla
nism for POS and for sets of similar words which                      and Teufel, 2006) and are planning on expanding
can be substituted into a string-based cue phrase                     this work to other cue phrases.
(Teufel, 1999). The grammar corresponds to 1762
cue phrases. It was developed on 80 papers which                      3.2   Cues Identified by annotators
are different to the papers used for our experiments                  During the annotator training phase, the annohere.                                                                 tators were encouraged to type in the metaThe other mechanism is a POS-based recog-                          description cue phrases that justify their choice of
niser of agents and a recogniser for specific actions                 category. We went through this list by hand and
these agents perform. Two main agent types (the                       extracted 892 cue phrases (around 75 per cateSpiegel-Rüsing found that out of 2309 citations she ex-         gory). The files these cues came from were not
amined, 80% substantiated statements.                                 part of the test corpus. We included 12 features
  Neut     PUse    CoCoGM     PSim    Weak     PMot     CoCoR0       PBas      CoCoXY     CoCo-    PModi     PSup
 62.7%    15.8%     3.9%      3.8%    3.1%     2.2%      0.8%        1.5%       2.9%      1.0%     1.6%      1.1%


                               Figure 3: Distribution of citation categories
     Weak CoCoGM CoCoR0            CoCo-     CoCoXY    PBas      PUse     PModi    PMot   PSim    PSup     Neut
 P     .78       .81      .77       .56        .72      .76       .66      .60      .75    .68     .83     .80
 R     .49       .52      .46       .19        .54      .46       .61      .27      .64    .38     .32     .92
 F     .60       .64      .57       .28        .62      .58       .63      .37      .69    .48     .47     .86
 Percentage Accuracy        0.77
 Kappa (n=12; N=2829; k=2) 0.57
 Macro-F                    0.57

      Figure 4: Summary of Citation Analysis results (10-fold cross-validation; IBk algorithm; k=3).


that recorded the presence of cues that our annota-                  Weakness Positive Contrast Neutral
                                                                 P      .80        .75       .77      .81
tors associated with a particular class.                         R      .49        .65       .52      .90
                                                                 F      .61        .70       .62      .86
3.3    Other features                                                 Percentage Accuracy        0.79
                                                                      Kappa (n=12; N=2829; k=2) 0.59
There are other features which we use for this                        Macro-F                    0.68
task. We know from Teufel and Moens (2002) that
verb tense and voice should be useful for recogniz-         Figure 5: Summary of results (10-fold crossing statements of previous work, future work and            validation; IBk algorithm; k=3): Top level classes.
work performed in the paper. We also recognise
modality (whether or not a main verb is modified                            Weakness Positive Neutral
                                                                        P       .77       .75     .85
by an auxiliary, and which auxiliary it is).                            R       .42       .65     .92
   The overall location of a sentence containing                        F       .54       .70     .89
a reference should be relevant. We observe that                         Percentage Accuracy        0.83
                                                                        Kappa (n=12; N=2829; k=2) 0.58
more PMot categories appear towards the begin-                          Macro-F                    0.71
ning of the paper, as do Weak citations, whereas
comparative results (CoCoR0, CoCoR-) appear                 Figure 6: Summary of results (10-fold crosstowards the end of articles. More fine-grained lo-          validation; IBk algorithm; k=3): Sentiment Analcation features, such as the location within the            ysis.
paragraph and the section, have also been implemented.
                                                            of {Weak, CoCoGM, CoCo-, CoCoR0, CoCoXY,
   The fact that a citation points to own previous
                                                            PBas, PUse, PModi, PMot, PSim, PSup, Neut}.
work can be recognised, as we know who the paThe papers are then further processed (e.g. toper authors are. As we have access to the inforkenised and POS-tagged). All other features are
mation in the reference list, we also know the last
                                                            automatically determined (e.g. self-citations are
names of all cited authors (even in the case where
                                                            detected by overlap of citing and cited authors);
an et al. statement in running text obscures the
                                                            then, machine learning is applied to the feature
later-occurring authors). With self-citations, one
                                                            vectors.
might assume that the probability of re-use of maThe 10-fold cross-validation results for citation
terial from previous own work should be higher,
                                                            classification are given in Figure 4, comparing the
and the tendency to criticise lower.
                                                            system to one of the annotators. Results are given
4 Results                                                   in three overall measures: Kappa, percentage accuracy, and Macro-F (following Lewis (1991)).
Our evaluation corpus for citation analysis con-            Macro-F is the mean of the F-measures of all
sists of 116 articles (randomly drawn from the part         twelve categories. We use Macro-F and Kappa beof our corpus which was not used for guideline              cause we want to measure success particularly on
development or cue phrase acquisition). The 116             the rare categories, and because Micro-averaging
articles contain 2829 citation instances. Each              techniques like percentage accuracy tend to overcitation instance was manually tagged as one                estimate the contribution of frequent categories in
heavily skewed distributions like ours 6 .                          5 Conclusion
   In the case of Macro-F, each category is treated
                                                                    We have presented a new task: annotation of cias one unit, independent of the number of items
                                                                    tation function in scientific text, a phenomenon
contained in it. Therefore, the classification sucwhich we believe to be closely related to the overcess of the individual items in rare categories
                                                                    all discourse structure of scientific articles. Our
is given more importance than classification sucannotation scheme concentrates on weaknesses of
cess of frequent category items. However, one
                                                                    other work, and on similarities and contrast beshould keep in mind that numerical values in
                                                                    tween work and usage of other work. In this
macro-averaging are generally lower (Yang and
                                                                    paper, we present machine learning experiments
Liu, 1999), due to fewer training cases for the rare
                                                                    for replicating the human annotation (which is recategories. Kappa has the additional advantage
                                                                    liable at K=.72). The automatic result reached
over Macro-F that it filters out random agreement
                                                                    K=.57 (acc=.77) for the full annotation scheme;
(random use, but following the observed distriburising to Kappa=.58 (acc=.83) for a three-way
tion of categories).
                                                                    classification (Weak, Positive, Neutral).
   For our task, memory-based learning outperWe are currently performing an experiment to
formed other models. The reported results use the
                                                                    see if citation processing can increase perforIBk algorithm with k = 3 (we used the Weka mamance in a large-scale, real-world information
chine learning toolkit (Witten and Frank, 2005)
                                                                    retrieval task, by creating a test collection of
for our experiments). Fig. 7 provides a few exresearchers’ queries and relevant documents for
amples from one file in the corpus, along with the
                                                                    these (Ritchie et al., 2006a).
gold standard citation class, the machine prediction, and a comment.                                                6 Acknowledgements
   Kappa is even higher for the top level distinction. We collapsed the obvious similar categories                   This work was funded by the EPSRC projects CIT-
(all P categories into one category, and all CoCo                   RAZ (GR/S27832/01, “Rhetorical Citation Maps
categories into another) to give four top level                     and Domain-independent Argumentative Zoncategories (Weak, Positive, Contrast,                               ing”) and SCIBORG (EP/C010035/1, “Extracting
Neutral; results in Fig. 5). Precision for all the                  the Science from Scientific Publications”).
categories is above 0.75, and K=0.59. For contrast, the human agreement for this situation was                   References
K=0.76 (n=3,N=548,k=3).                                             Rashid M. Abdalla and Simone Teufel. 2006. A bootstrapIn a different experiment, we grouped the cate-                     ping approach to unsupervised detection of cue phrase
                                                                       variants. In Proc. of ACL/COLING-06.
gories as follows, in an attempt to perform sentiSusan Bonzi. 1982. Characteristics of a literature as predicment analysis over the classifications:                                tors of relatedness between cited and citing works. JASIS,
                                                                       33(4):208–216.
                      Old Categories       New Category             Christine L. Borgman, editor. 1990. Scholarly CommunicaWeak, CoCo-        Negative                    tion and Bibliometrics. Sage Publications, CA.
 PMot, PUse, PBas, PModi, PSim, PSup       Positive                 Jean Carletta. 1996. Assessing agreement on classification
   CoCoGM, CoCoR0, CoCoXY, Neut            Neutral                     tasks: The kappa statistic. Computational Linguistics,
                                                                       22(2):249–254.
                                                                    Daryl E. Chubin and S. D. Moitra. 1975. Content analysis
   Thus negative contrasts and weaknesses are                          of references: Adjunct or alternative to citation counting?
grouped into Negative, while neutral contrasts                         Social Studies of Science, 5(4):423–441.
are grouped into Neutral. All positive classes                      Eugene Garfield. 1979. Citation Indexing: Its Theory and
                                                                       Application in Science, Technology and Humanities. J.
are conflated into Positive.                                           Wiley, New York, NY.
   Results show that this grouping raises results                   C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence. 1998.
to a smaller degree than the top-level distinction                     Citeseer: An automatic citation indexing system. In Proc.
                                                                       of the Third ACM Conference on Digital Libraries, pages
did (to K=.58). For contrast, the human agree-                         89–98.
ment for these collapsed categories was K=.75                       T.L. Hodges. 1972. Citation Indexing: Its Potential for Bibli-
(n=3,N=548,k=3).                                                       ographical Control. Ph.D. thesis, University of California
                                                                       at Berkeley.
     This situation has parallels in information retrieval,         David D. Lewis. 1991. Evaluating text categorisation. In
where precision and recall are used because accuracy over-             Speech and Natural Language: Proceedings of the ARPA
estimates the performance on irrelevant items.                         Workshop of Human Language Technology.
 Context                                                          Human         Machine     Comment
 We have compared four complete and three partial data rep-       PUse           PUse       Cues can be weak: “for... task...
 resentation formats for the baseNP recognition task pre-                                   presented in”
 sented in Ramshaw and Marcus (1995).
 In the version of the algorithm that we have used, IB1-IG,           Neut       PUse       Human decided citation was for
 the distances between feature representations are computed                                 detail in used package, not dias the weighted sum of distances between individual features                               rectly used by paper.
 (Bosch 1998).
 We have used the baseNP data presented in Ramshaw and                PUse       PUse       Straightforward case
 Marcus (1995).
 We will follow Argamon et al.               (1998) and use           PSim       PUse       Human decided F-measure was
 a combination of the precision and recall rates:                                           not attributable to citation. Hence
 F=(2*precision*recall)/(precision+recall).                                                 similarity rather than usage.
 This algorithm standardly uses the single training item clos-        Neut       PUse       Shallow processing by Machine
 est to the test i.e. However Daelemans et al. (1999) report                                means that it is mislead by the
 that for baseNP recognition better results can be obtained by                              strong cue in preceding sentence.
 making the algorithm consider the classification values of the
 three closest training items.
 They are better than the results for section 15 because more     CoCo-          PUse       Machine attached citation to the
 training data was used in these experiments. Again the                                     data set being used. Human atbest result was obtained with IOB1 (F=92.37) which is an                                   tached citation to the result being
 improvement of the best reported F-rate for this data set                                  compared.
 Ramshaw and Marcus 1995 (F=92.03).

                          Figure 7: Examples of classifications by the machine learner.


Terttu Luukkonen. 1992. Is scientists’ publishing behaviour           John Swales, 1990. Genre Analysis: English in Academic
   reward-seeking? Scientometrics, 24:297–319.                           and Research Settings. Chapter 7: Research articles in
Michael H. MacRoberts and Barbara R. MacRoberts. 1984.                   English, pages 110–176. Cambridge University Press,
  The negational reference: Or the art of dissembling. So-               Cambridge, UK.
  cial Studies of Science, 14:91–94.                                  Simone Teufel and Marc Moens. 2002. Summarising scientific articles — experiments with relevance and rhetorical
Michael J. Moravcsik and Poovanalingan Murugesan. 1975.
                                                                         status. Computational Linguistics, 28(4):409–446.
  Some results on the function and quality of citations. Social Studies of Science, 5:88–91.                                   Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006.
                                                                         An annotation scheme for citation function. In Proc. of
Greg Myers. 1992. In this paper we report...—speech acts                 SIGDial-06.
  and scientific facts. Journal of Pragmatics, 17(4).
                                                                      Simone Teufel. 1999. Argumentative Zoning: Information
John O’Connor. 1982. Citing statements: Computer recogni-                Extraction from Scientific Text. Ph.D. thesis, School of
   tion and use to improve retrieval. Information Processing             Cognitive Science, University of Edinburgh, UK.
   and Management, 18(3):125–131.                                     Peter D. Turney. 2002. Thumbs up or thumbs down? seChris D. Paice. 1981. The automatic generation of literary               mantic orientation applied to unsupervised classification
  abstracts: an approach based on the identification of self-            of reviews. In Proc. of ACL-02.
  indicating phrases. In R. Oddy, S. Robertson, C. van Rijs-          Melvin Weinstock. 1971. Citation indexes. In Encyclopebergen, and P. W. Williams, editors, Information Retrieval             dia of Library and Information Science, volume 5. Dekker,
  Research. Butterworth, London, UK.                                     New York, NY.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.             Howard D. White. 2004. Citation analysis and discourse
  Thumbs up? Sentiment classification using machine                      analysis revisited. Applied Linguistics, 25(1):89–116.
  learning techniques. In Proc. of EMNLP-02.                          Ian H. Witten and Eibe Frank. 2005. Data Mining: PractiAnna Ritchie, Simone Teufel, and Stephen Robertson.                      cal machine learning tools and techniques. Morgan Kauf2006a. Creating a test collection for citation-based IR ex-            mann, San Francisco.
  periments. In Proc. of HLT/NAACL 2006, New York, US.                Yiming Yang and Xin Liu. 1999. A re-examination of text
Anna Ritchie, Simone Teufel, and Stephen Robertson.                      categorization methods. In Proc. of SIGIR-99.
  2006b. How to find better index terms through citations.            John M. Ziman. 1968. Public Knowledge: An Essay ConIn Proc. of ACL/COLING workshop “Can Computational                     cerning the Social Dimensions of Science. Cambridge
  Linguistics improve IR”.                                               University Press, Cambridge, UK.
Simon Buckingham Shum. 1998. Evolving the web for scientific knowledge: First steps towards an “HCI knowledge
  web”. Interfaces, British HCI Group Magazine, 39.
Henry Small. 1982. Citation context analysis. In P. Dervin
  and M. J. Voigt, editors, Progress in Communication Sciences 3, pages 287–310. Ablex, Norwood, N.J.
Ina Spiegel-Rüsing. 1977. Bibliometric and content analysis. Social Studies of Science, 7:97–113.
John Swales. 1986. Citation analysis and discourse analysis.
   Applied Linguistics, 7(1):39–56.